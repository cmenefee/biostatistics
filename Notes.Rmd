---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Project Details {.tabset .tabset-fade .tabset-pills} 
## The Book
Methods in Biostatistics with R Ciprian Crainiceanu, Brian Caffo, John Muschelli

 * Intro Video: https://www.youtube.com/watch?v=MG1lZhixbS4
 * Site: https://leanpub.com/biostatmethods

## The Data
From the Book

 * https://github.com/muschellij2/biostatmethods









# Chapter 1. Introduction

"....We provide a modern look at introductory biostatistical concepts and associated computational tools, reflecting the latest developments in computation and vi- sualization using the R language environment (R Core Team 2016). The idea is to offer a complete, online, live book that evolves with the newest developments and is continuously enriched by additional concepts..."

1.1 Biostatistics 

"....Biostatistics is the theory and methodology for the acquisition and use of quantitative evidence in biomedical research. 
Biostatisticians develop innovative designs and analytic methods targeted at increasing available information, 
improving the relevance and validity of statistical analyses, making best use of available information and communicating relevant uncertainties....”

"....Biostatistics is a challenging subject to master if one is interested in understanding both the underlying concepts and their appropriate implementation. 
....Biostatistics can be viewed as a powerful and practical philosophy of science, in which the scientific hypothesis, 
the experiment, the data, the model, and the associated inference form the basis of scientific progress. 
....Biostatistics is hard because it is focused on solving difficult problems using simple approaches and simplicity is difficult to achieve in complex scenarios.
....Biostatistics is not hard in the usual sense that it requires either deep mathematical concepts or committing large books to memory...." 

"....The difficulty of biostatistics rests in the different philosophy, the different way of thinking, 
and the different set of skills required to process human communication of scientific problems and their translation into well-defined problems that can be solved with data. 
Once a scientific promlem is defined, the biostatistics philosophy is to try to solve it using the simplest possible approaches that are not too simplistic. 
Thus, parsimony and inductive reasoning are fundamental concepts in biostatistics...."

"....biostatistics starts with the current accepted state of knowledge (the collection of null hypotheses) and uses data to inductively refute or reinforce
parts of the knowledge or generate new potential knowledge (new null hypotheses). In the biostatistical philosophy there is no truth, just the state of current knowledge...."

"....Biostatistics requires: 
(1) a tight coupling of the biostatistical methods with ethical and scientific goals of research; 
(2) an emphasis on the scientific interpretation of statistical evidence to impact policy; and 
(3) a detailed acknowledgment of assumptions and a comprehensive evaluation of the robustness of conclusions to these assumptions...."



# Chapter 2. Introduction to R


## 2.4 Objects In R

"....Almost everything is an “object” or “variable.”....most things are relevant only with respect to a data set...."

## 2.5 Assignment
"....There is also a “forward assignment” operator that exists, but this is rarely used and we do not recommend using it...."
Oh no.

## 2.6 Data types In R

"....The three most fundamental: numeric (numbers), character (strings or “words”), and logicals (TRUE or FALSE). 

"....One additional data type: factors. Factors are categorical data types. 
The categories of a factor are called the “levels” of that factor. The levels of factors usually contain human-readable levels (e.g., “Male” and “Female”). 
Factors become increasingly useful when we fit models, as one can define the baseline category by the levels of the factor, which affects the interpretation of coefficients. 
Some refer to vectors of length 1 as a “scalar”. In other languages, there is a distinction between scalars and vectors, but not in R. 
So you can think of a vector as a 1-dimensional object that can have anywhere from 0 to a large number of elements...."

## 2.7.1 More than one number: vectors In R

"....Anything having one dimension is generally referred to as a “vector”. A vector has a specified length. Technically, a vector may have length 0, which means nothing is in it. 
But usually vectors have one or multiple objects in them. Those objects all must have the same data type (e.g., all numbers or all characters or all logicals)...."

## 2.7.3 Sequences

````{r}
1:5
-5:1
-(5:1)
seq(1,5)
seq(1,7, by = 0.4)
seq(by = 0.4, to = 7, from = 1)


````


## 2.7.4 Operations on numeric vectors

````{r}
x_length_2 = c(1, 2) 
x_length_2 * 3
sqrt(x_length_2)

````


## 2.7.5 Some standard operations on vectors 

````{r}
x_length_3 = c(10, 20, 30)
length(x_length_3)
x = c(2, 1, 1, 4)
sort(unique(x))
sample(x)
sample(x, size = 2)

# sample(x, size = 10) will error 
sample(x, size = 10, replace = TRUE)

````



## 2.7.7 Data type coercion 
"....Vectors can only have one data type in them...."

"....Logicals can be coerced to numerics (0 and 1 values) and coerced to characters ("TRUE" and "FALSE", note the quotes). 
Numerics can be coerced into characters (5.6 changes to "5.6")...."

You can determine the data type of an object with the typeof() function: 

````{r}
typeof(c(5.2, TRUE, FALSE))

````

## 2.7.8 More than one dimension: Matrices 

"....A matrix is like a vector, but it has two dimensions, rows and columns. Like vectors, matrices can contain only one data type...."
So I imagine a vector is just 'A row' and a matrix is 'A row and a column', or something like that

````{r}
matrix(1)
matrix(1:6, nrow = 2, ncol = 3) 
matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE) # filled by row

````

## 2.7.9 More than one dimension: data.frames 

"....A data.frame is like a matrix in that it has rows and columns. A data.frame can have columns of different data types...."

....Note, a default on the data.frame() function is stringsAsFactors = TRUE, which indicates that all strings should be converted to factors. 
Many times, you want to clean these columns and then convert them to factors directly. 
So we will set that option to FALSE...."


````{r}
df = data.frame(age = c(25, 30, 32, 42), 
                handed = c("left", "right", "ambidextrous", "left"), 
                stringsAsFactors = FALSE)
````


## 2.7.10 Dimensions of an object 

There are functions that get attributes about a matrix/data.frame that are useful for analysis. 

The dim function returns the dimensions (rows and columns, respectively) of a data set: 
````{r}
df = data.frame(age = c(25, 30, 32, 42), 
                handed = c("left", "right", "ambidextrous", "left"), 
                stringsAsFactors = FALSE)
dim(df) 

````

Number of rows or columns
````{r}
df = data.frame(age = c(25, 30, 32, 42), 
                handed = c("left", "right", "ambidextrous", "left"), 
                stringsAsFactors = FALSE)
nrow(df)
ncol(df)

````

## 2.8 Logical operations 

"....Perform some logical tests that return logical data. 
greater/less than (>, <)
greater/less than or equal to (>=, <=)
equal to (== “double equals”)
and not equal to (!=)
These are called “relational operators” or “comparison operators”

There are also the standard “logical operators”: 
not/negate (!), 
AND (&), and 
OR (|)...." 

````{r}
x = c(1, 3, 4, 5) 
x < 4
x > 3
x >= 3
x == 3
x != 3
x = c(1, 3, 4, 5) 
x > 3 | x == 1 
!(x > 3 | x == 1) # negates the statement - turns TRUE to FALSE [1] FALSE TRUE FALSE FALSE 
x != 3 & x != 5
x == 3 | x == 4

````

## 2.8.1 The %in% Operator 

"....Many times, we want to keep or exclude based on a number of values...." 

````{r}
x = c(1, 3, 4, 5) 
x %in% c(3,4)

````
This feels like an "is in?" or may even an "is?" in that it asks: "are any of these values in the vector?".



## 2.9 Subsetting 2.9.1 Subsetting vectors

````{r}
x = c(1, 3, 4, 5)
x[4]
x > 2
x[ x > 2 ]

````

## 2.9.2 Subsetting matrices

"....the syntax is [row subset, column subset]...."

````{r}
mat = matrix(1:6, nrow = 2)
mat[1:2, 1]
mat[1, 1:2]
mat[1, c(FALSE, TRUE, FALSE)]
mat[, c(FALSE, 3, TRUE)]
mat[1:2, ]
mat[, 2:3]
mat[, c(FALSE, TRUE, FALSE)]

````

"....You can ensure that the result is a matrix, even if the result is one-dimensional using the drop = FALSE argument...."
````{r}
mat[,1, drop = FALSE]

````


## 2.12.1 Tibbles 

Implemented in the tibble package. It is like a data.frame

* Never coerces inputs (i.e. strings stay as strings!). 
* Never adds row.names. 
* Never changes column names. 
* Only recycles length 1 inputs. (no wraparound) 
* Automatically adds column names.

````{r}
tbl = tibble::as_tibble(df)

````



## 2.13 Problems 



### Problem 1. Consider the vector: num <- c("1.5", "2", "8.4")
a. Convert num into a numeric vector using as.numeric 
````{r}
num <- c("1.5", "2", "8.4")
as.numeric(num)

````

b. Convert num into a factor using factor, calling it num_fac. 
````{r}
num <- c("1.5", "2", "8.4")
num_fac <- factor(num)

````

c. Convert num_fac into a numeric using as.numeric. 
````{r}
as.numeric(num_fac)

````

d. Convert num_fac into a numeric using as.numeric(as.character()) 
````{r}
as.numeric(as.character(num_fac))

````



### Problem 2. Consider the vector: num <- c(0, 1, 1, 0) 
a. Append TRUE to the num vector using c. 
````{r}
num <- c(0, 1, 1, 0) 
num <- c(num, TRUE)
num

````

b. Check the class of num using class. 
````{r}
num <- c(0, 1, 1, 0) 
class(num)

````

c. Convert num into a logical vector using as.logical. 
````{r}
num <- c(0, 1, 1, 0) 
as.logical(num)
````

d. Convert num into a logical vector where the result is TRUE if num is 0, using the == operator. 
````{r}
num <- c(0, 1, 1, 0) 
as.logical(num == 0)

````



### Problem 3. Consider the data set data_bmi from above. Perform the following operations: 
a. Extract the column nams of data_bmi using the colnames() function. 
````{r}
file_name = file.path("Example_Data/bmi_age.txt")
data_bmi = read.table(file = file_name, header = TRUE, stringsAsFactors = FALSE) # found elsewhere in the book. Downloaded from the repository
colnames(data_bmi)

````

b. Subset the AGE column using data_bmi[, "AGE"]. 
````{r}
file_name = file.path("Example_Data/bmi_age.txt")
data_bmi = read.table(file = file_name, header = TRUE, stringsAsFactors = FALSE)
data_bmi[, "AGE"]

````

c. Subset the AGE column using data_bmi$AGE. 
````{r}
file_name = file.path("Example_Data/bmi_age.txt")
data_bmi = read.table(file = file_name, header = TRUE, stringsAsFactors = FALSE)
data_bmi$AGE

````

d. Subset the AGE and BMI columns using brackets and the c() function.
````{r}
file_name = file.path("Example_Data/bmi_age.txt")
data_bmi = read.table(file = file_name, header = TRUE, stringsAsFactors = FALSE)
data_bmi[c("AGE", "BMI")]

````




### Problem 4. Here we will work with sequences and lengths: 
a. Create a sequence from 1 to 4.5 by 0.24. Call this run_num. 
````{r}
run_num <-seq(from = 1, to = 4.5, by = 0.24)


````

b. What is the length of run_num. 
````{r}
length(run_num)

````

c. Extract the fifth element of run_num using brackets.
````{r}
run_num[5]

````


### Problem 5. Let’s create a tibble called df:
a. Extract the column x using the $.
````{r}
df = dplyr::data_frame(x = rnorm(10), y = rnorm(10), z = rnorm(10)) 
df$x

````

b. Extract the column x using the [,] notation.
````{r}
df = dplyr::data_frame(x = rnorm(10), y = rnorm(10), z = rnorm(10)) 
df[,"x"]
````

c. Extract columns x and z. 
````{r}
df = dplyr::data_frame(x = rnorm(10), y = rnorm(10), z = rnorm(10)) 
df[c("x", "y")]

````

d. Extract the third and fifth rows of df and columns z and y.
````{r}
df = dplyr::data_frame(x = rnorm(10), y = rnorm(10), z = rnorm(10)) 
df[c(3,5),]
df[c("x","y")]
````




### Problem 6
a. Get the mean of the column x using the $ operator. 

````{r}
df = dplyr::data_frame(x = rnorm(10), y = rnorm(10), z = rnorm(10))
mean(df$x)



````


b. Pipe (%>%) the column x into the mean function. 

````{r}
library(dplyr)
df$x %>% mean

````

c. Look at the summarize function to look ahead to future chapters and try to do this in the dplyr framework.

````{r}
print(paste("K"))
````

### Problem 7.
Consider the data set data_bmi using the BMI data, but read in using readr: 

````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name) 
````
a. What is the class of data_bmi? 
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
class(data_bmi)
````



b. What is the class of data_bmi[, "AGE"]? 
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
class(data_bmi[, "AGE"])
````


c. What is the class of data_bmi$AGE? 
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
class(data_bmi$AGE)
````



d. What is the class of data_bmi[, "AGE", drop = TRUE]?
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
class(data_bmi[, "AGE", drop = TRUE])
````



### Problem 8. Consider the data set data_bmi using the BMI data, but read in using readr: 
file_name = "bmi_age.txt" 
data_bmi = readr::read_table2(file = file_name) 
Parsed with column specification: 

cols( PID = col_double(), BMI = col_double(), SEX = col_double(), AGE = col_double() ) 

a. What is the mean of the AGE column? 
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
mean(data_bmi$AGE)
````



b. Set the 3rd element of data_bmi$AGE to be 42?
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
data_bmi$AGE[3] = 42
````
c. What is the mean of the AGE column now?
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
mean(data_bmi$AGE)
````


### Problem 9. Use data_bmi from the above problem: 

a. Remove the X5 column using data_bmi$X5 = NULL 
````{r}
file_name = "Example_Data/bmi_age.txt"
data_bmi = readr::read_table2(file = file_name)
data_bmi$X5 = NULL
````



b. Create mat, which is data_bmi as a matrix, using as.matrix. 
````{r}
mat <- as.matrix(data_bmi)
````



c. Try to extract AGE from mat using the $. What happened? 
````{r}
#It causes a fatal error
#mat$AGE
````



d. Extract AGE from mat using the [,] notation.
````{r}
mat[,"AGE"]
````


 
# Chapter 3. Probability, random variables, distributions

## 3.1 Experiments 

"....An experiment is the process of data collection for a target population according to a specific sampling protocol that includes rules for what, when, where, and how to collect data on experimental units (e.g. individuals) from the target population...."

Consider the outcome of an experiment such as: 

* a collection of measurements from a sampled population 
* measurements from a laboratory experiment 
* the result of a clinical trial 
* the results of a simulated (computer) experiment 
* values from hospital records sampled retrospectively



## 3.1.1 Notation

To develop these concepts rigorously, we will introduce some notation for the experiment setup. 
* The sample space Ω is the collection of possible outcomes of an experiment Example: a six-sided die roll = {1; 2; 3; 4; 5; 6}, a coin flip = {heads; tails} 
* An event, say E, is a subset of Ω Example: die roll is even E = {2; 4; 6} 
* An elementary or simple event is a particular result of an experiment Example: die roll is a four, ω = 4 
* ∅ is called the null event or the empty set

"....Biostatis- tics is concerned with extracting useful, actionable information from complex outcomes that can result from even the simplest experiments...."



## 3.1.2 Interpretation of set operations 
"....Data analysis, manipulation, and tabulation are intrinsically related to data sets and logic operators. 
Indeed, we often try to understand the structure of the data and that requires understanding how to subset the data and how to quantify the various relationships between subsets of the data. To better understand this, we need to introduce the theoretical set operations and their interpretations...." 

* ω ∈ E implies that E occurs when ω occurs 
* ω ∈/E implies that E does not occur when ω occurs 
* E ⊂ F implies that the occurrence of E implies the occurrence of F (E is a subset of F ) 
* E ∩ F implies the event that both E and F occur (intersection) 
* E ∪ F implies the event that at least one of E or F occur (union) 
* E ∩ F = ∅ means that E and F are mutually exclusive/disjoint, or cannot both occur 
* E c or Ē is the event that E does not occur, also referred to as the com- plement
* E \ F = E ∩ F c is the event that E occurs and F does not occur


## 3.1.3 Set theory facts 

DeMorgan’s laws. 

* (A ∪ B)^c^ = Ac ∩ B^c^
* (A ∩ B)^c^ = Ac ∪ B^c^
* (A^c^)^c^ = A 
* (A ∪ B) \ C = (A \ C) ∪ (B \ C) where \ means “set minus”


"...Proving the equality of two sets, A = B, is done by showing that every element of A is in B (i.e., A ⊂ B) and every element in B is in A (i.e., B ⊂ A)...."

"....In general, set operations are based on logical operators: AND (in R &), OR (in R | ), NOT (in R !). These logical operators can be combined and can produce complex combinations that are extremely useful in practice. For example, hav- ing a data set one may want to focus on a subset that contains only African American men from age 65 to 75 who are non-smokers. This phrase can im- mediately be translated into logical operators and the data can be extracted using R programming. This is a routine application of set theory that will be- come indispensable in practice (below we provide an example of exactly how to proceed). 

"....We start by reading a small data set. First, let us tell R where the file is: 
The data are loaded using the read.table function We specify arguments/options to read.table that the file contains a header row (header = TRUE) and we do not want to convert strings/words to a categorical variable/factor when the data are read in (stringsAsFactors = FALSE)....We can simply write the object name out to show the data (here the entire data set is displayed because it is small)...." 

````{r}
file_name = file.path("Example_Data/bmi_age.txt")
data_bmi = read.table(file = file_name, header = TRUE, stringsAsFactors = FALSE)
data_bmi #show the data
head(data_bmi) #show only a portion of the data
dim(data_bmi) #display the dimension of the data
colnames(data_bmi) #the name of the variables (column names):
mean(data_bmi$BMI)  
sd(data_bmi$BMI) #standard deviation
````


"....For simplicity, assign all the columns of the dataset to their own separate variables using the attach function. We do not recommend this approach in general, especially when multiple datasets are being used...."

````{r}
attach(data_bmi)

````



A basic plot of age vs. BMI:
````{r}
plot(AGE, BMI,type="p",pch=20,cex=2,col="blue") 
rect(xleft = 45, xright = 100, ybottom = 0, ytop = 26, col = scales::alpha("purple", 0.5)) 
rect(xleft = 45, xright = 100, ybottom = 26, ytop = 100, col = scales::alpha("red", 0.5)) 
rect(xleft = 0, xright = 45, ybottom = 0, ytop = 26, col = scales::alpha("blue", 0.5)) 
points(AGE, BMI,type="p",pch=20,cex=2,col="blue") 
text(x = AGE + 1, y = BMI, labels = PID, col = "black") 
abline(h = 26, v = 45, col = "RED")
````


"....Here the call to abline is for drawing vertical and horizontal lines so that we can count the numbers in each respective box. We now make the connection between set theory and operations clearer in the context of the data. In particular, we emphasize how set operations translate into logic operators that can then be used for data extraction and operations. Consider the following subsets of subjects: 

A subjects with BMI< 26 and B subjects older than 45 (AGE> 45). 
Construct a logical indicator for which records fall into A...."

````{r}
#Group A, #represented by the points in the purple/blue regions
index_BMI_less_26 = BMI < 26 
#Group B, #represented by the points in the purple/red regions 
index_AGE_greater_45 = AGE > 45 
````


"....Display the IDs for A and B. Here PID is the unique patient ID, though in many applications the PID can be more complex than just integer numbers...." 
````{r}
PID[index_BMI_less_26]
PID[index_AGE_greater_45]
````



Let us calculate (A ∩ B)^c^ , the complement of the intersection between A and B, which is shown by the non-purple regions in Figure 3.1. These are subjects who do not/are (note the c that stands for complement) (have a BMI less than 26) and (older than 45) and 

````{r}
index_A_int_B_compl = !(index_BMI_less_26 & index_AGE_greater_45) 
PID[index_A_int_B_compl]
````


The translation of (A∩B^c^ into R is !(index_BMI_less_26 & index_AGE_greater_45). Note that ! indicates is not, or complement, and & indicates and, or intersection. 
So, the resulting IDs are everybody, except the subject with IDs 4 and 7. 


It would be instructive to conduct the same type of analysis for 


A^c^ ∪ B^c^ = (A ∪ B)^c^
````{r}
index_A_int_B_compl = (index_BMI_less_26 & index_AGE_greater_45) 
PID[index_A_int_B_compl]
````



(A ∪ B)^c^ , 
````{r}
index_A_int_B_compl = (index_BMI_less_26 & index_AGE_greater_45) 
PID[index_A_int_B_compl]
````



and A^c^. 
````{r}
index_A_int_B_compl = (index_BMI_less_26)
PID[index_A_int_B_compl]
````



## 3.2 An intuitive introduction to the bootstrap

"....A major problem in practice is that, even if we run two identical experiments, data are never the same.

However, even though the two samples will be different, they will have some things in common. 
Those things are the target of estimation, the probability and time for conversion from being healthy to developing lung cancer, and the original target population...."

In practice we often have one study and we are interested in understanding what would happen if multiple studies were conducted. 
The reason for that is fundamental and it has to do with the generalizability of the experiment. 
Indeed, a study would collect data for a subsample of the population to make predictions about the rate and expected time of transition to lung cancer in the overall population. Bootstrap is a widely used statistical technique based on resampling the data. 
The bootstrap is designed to create many potential studies that share the same characteristics with the study that collected the data and may represent the vari- ability of running these experiments without actually running the experiments...." 

"....The nonparametric bootstrap is the procedure of resampling with replacement from a dataset, where the number of observations in each resampled dataset is equal to the number of observations in the original dataset...."

So this is pretty much "take multiple random samples, the game" - Oh and here's one way to do it:

````{r}
set.seed(4132697) 
nboot=3 #number of boostrap samples 
nsubj=nrow(data_bmi) #number of subjects 
for (i in 1:nboot) #start bootstrapping 
  {#begin the bootstrap for loop 
    resampling = sample(nsubj, replace=TRUE) #sample with replacement 
    bootstrapped_data = data_bmi[resampling,] #resample the data set 
    print(bootstrapped_data) #print bootstrapped datasets 
    cat(" \n \n") #add lines between datasets }#end the bootstrap for loop
  }#end the bootstrap for loop
````

"....A close look at these bootstrap samples will indicate what is meant by creating many potential studies that share the same characteristics. 
The subjects in every bootstrap sample are the same as those in the data, though some may not appear and others may appear multiple times. 
All information about the subject is preserved (information across columns is not scrambled), and the number of subjects in each bootstrap sample is the same as the number in the original dataset. This was done by using sampling with replacement of the subjects’ IDs using the same probability. 
Sampling with replacement can be conceptualized as follows: 

 1) consider a lottery machine that contains identical balls that have the name, or IDs, of n subjects; 
 2) sample one ball, record the ID and place the ball back into the lottery machine; 
 3) repeat the experiment n times...."
 
 ....Here we set the seed and the same exact boostrap samples are obtained every time this book is compiled. However, in practice if you run the code multiple times without the set.seed() option do not be surprised that every time the 3 bootstrap data sets change. 
This happens because of the random nature of the sampling mechanism. The total number of different nonparametric bootstrap datasets is 1010 , or 10 billion. 
If you want to always obtain the same dataset you can set the seed (this would need to be done before the sampling code) of the pseudo-random number (PRN) generator in R set.seed(4132697) The set.seed() function resets the PRN for the next simulation and does not have an effect on the previous simulation...."

